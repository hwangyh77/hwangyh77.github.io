---
title:  "AdaBoost"
categories:
  - computervision
---

# AdaBoost  

AdaBoost는 Additive Boosting의 줄임말로, 1995년에 등장하였지만 빠르고 정확한 성능으로 좋은 평가를 받고 있는 알고리즘이다.  

## < Bootstrapping >  

통계학에서의 부트스트랩과 다른 점도 있지만, 본질적으로 같다고 할 수 있다.   
통계학적으로는 정확한 분포를 모르는 데이터의 통계치의 분포를 알아내기 위하여 Random Sampling을 하는 경우를 말하며, 종종 측정된 샘플이 부족한 경우에도 사용된다.  
기계학습에서는 기본적으로 Random Sampling을 통해 데이터의 수를 늘리는 것을 말한다.  
<font color='red'>(Used to evaluate the performance of a classifier and estimate function regression)</font>  

![image](https://user-images.githubusercontent.com/93988405/235329637-6cb7ce2f-a481-459f-9e3e-d03ab5a979cd.png){:class="align-center"}  
<중복을 허용해서 N개를 B세트만큼 뽑는다.>{:class="align-center"}  

## < Bagging >  

Bagging은 Bootstrap Aggregatint의 줄임말로 부트스트랩이 over-fitting을 줄이는 데에 사용될 때를 말한다.  
여러 개의 Bootstrap을 생성 즉, 주어진 데이터에 대해 여러 번의 Random Sampling을 통해 Training Data를 추출하고 
독립된 모델로서 각각의 자료를 학습시키고 이를 앙상블로서 결합하여 최종적으로 하나의 예측 모형을 산출하는 방법이다.  
<font color='red'>Classifier의 stability를 높이고 분포도를 줄인다!!</font>  

## < Boosting >  

이전의 잘못을 파악하고 이를 이용하여 다음 번에는 더 나은 모델을 만들어 내자는 목표를 추구하면서 학습하는 방법이다.  

![image](https://user-images.githubusercontent.com/93988405/235329673-cb887ba6-8d3c-4930-96d5-50dcb6461519.png){:class="align-center"}  

- 중복없이 N개보다 적게 뽑아 C1 학습  
- 실패한 sample들의 반을 가져와 N개보다 적게 구성하여 C2 학습  
- C1과 C2의 의견이 다른것들 중 sample을 구성해 C3학습  

## < AdaBoost Algorithm >  

AdaBoost는 t개의 weak(base) classifier를 반복적으로 호출하여 학습을 진행한다. (t = 1 ~ T)  
> Learner = Hypothesis = Classifiers 이며  
> 
> Weak Classifier = Base Learner이다.  

![image](https://user-images.githubusercontent.com/93988405/235329707-cc869d62-6276-46df-a718-3901ca1aca36.png){:class="align-center"}  

- 샘플들의 weights를 구한다. (weight들의 합 = 1)  
- weighted training samples들의 weak classifier 만들기  
- Combine weak classifiers linearly -> weighted vote of weak classifiers  

![image](https://user-images.githubusercontent.com/93988405/235329731-fa21ce7a-9bc2-4780-b2aa-a90ff872ac88.png){:class="align-center"}  

- et = i ~ t 까지의 error  
- et 값이 작아질수록 at 값이 커진다.(et ≤ 0.5 이면 at ≥ 0)  

따라서 at를 면 et 값이 작아지면 값이 커진다. => weight 값 만들어주는 방법!!(틀릴수록 가중치가 커지는)  

AdaBoost는 다음과 같이 작동한다.  

![image](https://user-images.githubusercontent.com/93988405/235329888-bbab4647-0b18-47f2-9fe7-574a6d46eb4a.png){:class="align-center"}  

위의 AdaBoost algorithm 에러를 구하는 식대로 e와 a를 구할 수 있다.  
틀린것에 가중치를 더 주는 방식을 틀린 부호가 커지고 진해지는 것으로 표현하였다.  
이렇게 구성된 각각의 Classifier들을 아래와 같은 방법으로 Strong Classifier로 출력한다.  

![image](https://user-images.githubusercontent.com/93988405/235329899-051cb758-a791-488c-9a66-8fac0d206fba.png){:class="align-center"}  

## < Confusion Matrix >  

Confusion Matrix란 Training을 통한 Prediction 성능을 측정하기 위해 예측 value와 실제 value를 비교하기 위한 표이다.  

![image](https://user-images.githubusercontent.com/93988405/235329995-e06df36b-6f28-4c6f-9108-ab4294c71935.png){:class="align-center"}  

> - TP = 얼굴이라 판단했는데 맞은거  
> - TN = 얼굴이 아니라 판단했는데 맞은거  
> - FP = 얼굴이라 판단했는데 틀린거 ( = 얼굴이 아닌데 얼굴이라 한것 )  
> - FN = 얼굴이 아니라 판단했는데 틀린거 ( = 얼굴인데 얼굴이 아니라 한것 )  

위의 Matrix로 아래와 같이 여러 가지 성능지표를 표현할 수 있다.  

![image](https://user-images.githubusercontent.com/93988405/235330001-24835c3b-c9cd-4ec8-bfb5-fd86d54234dd.png){:class="align-center"}  

