---
title:  "StyleGAN 정리"
categories:
  - deeplearning
---

# StyleGAN  
  
방학동안 여러 GAN을 공부하였고 공부한 내용을 바탕으로 프로젝트를 진행하게 되었다.  
처음에는 DCGAN과 StarGAN의 특징을 적절히 섞어 latent space에서의 산술연산과 여러 domain간의 이동이 가능한 GAN모델을 만드려고 하였다.  
하지만 공부를 진행하며 내가 원하던 모델과 거의 똑같은 StyleGAN이 있다는 것을 알 수 있었고 이에 StyleGAN에 대해 공부를 하였다.
  
  
## < StyleGAN에 대하여 >  
  
Generator를 통한 이미지 합성 과정은 여전히 block box로 여겨지며, 이로 인해 합성되는 이미지의 
attribute (성별, 연령, 헤어스타일 등) 을 조절하기가 매우 어렵다는 한계를 해결하기 위해 
Style Transfer에 기반한 새로운 generator 구조인 StyleGAN을 제안. StyleGAN은 이미지를 
style의 조합으로 보고, generator의 각 layer 마다 style 정보를 입히는 방식으로 이미지를 합성한다. 
이 때 style은 이미지의 coarse feature(성별, 포즈등), fine detail(머리색, 피부톤 등)까지 
여러 level의 visual attribute를 조절할 수 있다.  

## < 프로젝트의 가능성 >  

온라인 쇼핑몰에서 옷을 살 때 피팅 모델이 그 옷을 입고 있는 모습을 보고 나에게 어울릴지 생각해본 뒤,
사이즈를 확인하고 살지 말지 결정한다. 하지만 모델의 체형과 나의 체형이 완벽히 같지 않기 때문에 
옷의 사진만 보고 샀을 때 뭔가 잘 안어울려서 잘 안입게 되는 경우가 비일비재하다. 
하지만 StyleGAN을 이용하여 자신의 사진을 이용하여 원하는 옷을 대입시켜 
나에게 잘 맞는지 미리 확인해볼 수 있을 것이다.  

또는 집의 사진을 찍어 내가 원하는 가구가 집에 들어오면 어떤 느낌일지 미리 파악해 볼 수 있으며
집에 어울리는 컬러감, 외형 등을 선택하기가 수월해질 것이다.  

## < StyleGAN의 특징 >  

![image](https://user-images.githubusercontent.com/93988405/219857733-63623000-734c-474c-8b00-fa028570a33c.png){:class="align-center"}  

기존의 generator (a)는 input latent vector (z)가 직접 convolution, upsampling 등을 거쳐 이미지로 변환되는 구조이다.
그러나 style-based generator (b) 의 경우, input vector가 직접 convolution으로 들어가지 않고 **fully connected layer로 구성된 
mapping network를 통과하여** <u>비선형 w vector로 변화 시킨다.</u> 
즉, (z)가  fully-connected layer로 구성된 mapping network을 거쳐 
intermediate latent vector (w)로 먼저 변환된다. mapping network를 사용하여 
입력 z vector를 각각의 요소가 다른 visual feature를 control하게 만드는 intermediate vector로 변환시킨다.  

이때 intermediate vector w는 정규분포를 따를 필요가 없다. 
그래서 data set을 훨씬 유동적인 공간(intermediate latent space)에 mapping 할 수 있게 된다. 
(w는 constant tensor가 이미지로 변환되는 과정에서 스타일을 입히는 역할을 수행함으로써 
다양한 스타일의 이미지를 만들어낼 수 있다.)  

## < Mapping Network >  

StyleGAN의 가장 큰 특징은 input vector z 로부터 직접 이미지를 생성하는 게 아닌 mapping network를 거쳐 intermediate vector w로 먼저 변환한후 이미지를 생성한다는 점이다.  

![image](https://user-images.githubusercontent.com/93988405/219857885-2d1d1bd1-d41e-45cf-bee9-e3e753cfe006.png){:class="align-center"}  

기존의 방법처럼 input vector로부터 이미지를 직접 생성할 때 고정된 input distribution에 학습 이미지의 distribution을 맞춰야 한다는 한계가 있다. 
이러한 한계로 인해 visual attribute가 input space에 non-linear하게 mapping 되므로 input vector로 visual attribute를 조절하기가 매우 어렵다.
하지만 mapping network를 사용할 경우 w는 고정된 distribution을 따를 필요가 없어지기 때문에,
**학습 데이터를 훨씬 유동적인 공간 (intermediate latent space)에 mapping할 수 있고 w를 이용해 visual attribute를 조절하기가 훨씬 용이해진다.**
<font color='red'>이러한 특징을 disentanglement라 한다.</font>  

> **< AdaIN(Style Module) >**  
>
> 빠른속도의 추론이 가능하면서 동시에 Arbitrary하게 새로운 스타일을 적용할 수 있는 방식이다.  
>
> ![image](https://user-images.githubusercontent.com/93988405/219858042-fb283674-ad20-4f1f-bc54-9285d45f70d6.png){:class="align-center"}  
>
> 녹색의 VGG의 pre-trained 모델을 통해서 Encoding을 수행하며, 이 encoder를 feature를 인코딩할 때, 그리고 Loss Function을 구할 때 사용한다는 것이다. 즉 Encoder는 학습 시키지 않는다는 점이 포인트다.
> 그러므로, 자연스럽게 이 네트워크 상에서 학습 시키는 것은 Decoder 뿐이며, 
> 저자들의 표현을 빌리자면 이 Decoder는 AdaIN으로 생성된 feature들이 decoder를 통해서 image space로 invert 하는 법을 학습한다. 
> 아직 설명하지 않았지만, AdaIN 내에서는 learnable parameter가 없다.  

위에서 말했던 w는 synthesis network가 이미지를 생성하는 과정에서 style을 입히는데 사용된다. 
Synthesis network는 4x4x512 constant tensor를 convolution, upsampling을 통해 1024x1024x3 이미지로 변환시킵니다. 
이 때 각 convolution layer 이후마다 Adaptive Instance Normalization (AdaIN) 을 통해 style이 입혀지며, 
style vector (y)는 w로부터 affine transformation을 통해 얻는다.  

![image](https://user-images.githubusercontent.com/93988405/219858461-3864d156-9da0-4373-884b-972d139fe6c2.png){:class="align-center"}  

이러한 구조는 다음의 특징을 지닌다.  

- Synthesis network의 매 layer마다 AdaIN을 통해 style을 normalize 한 후 새로운 style을 입히게 되므로, 특정 layer에서 입혀진 style은 바로 다음 convolutional layer에만 영향을 준다. 따라서 각 layer의 style이 특정한 visual attribute만 담당하는 것이 용이해진다.  

- Style을 조정한다는 것은 이미지의 global한 정보를 통째로 조정한다는 것을 의미한다. 이로 인해 항상 spatially-consistent한 이미지를 얻게 되고, 기존의 generator보다 훨씬 안정적으로 자연스러운 이미지를 얻을 수 있다.  

## < References >  

https://blog.lunit.io/2019/02/25/a-style-based-generator-architecture-for-generative-adversarial-networks/

https://velog.io/@tobigs-gm1/Style-GAN
