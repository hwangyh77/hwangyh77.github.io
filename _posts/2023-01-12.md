---
layout: post
title:  "Image-to-Image Translation with Conditional Adversarial Networks 논문 공부"
---

# Image to Image Translation with Conditional Adversarial Networks  

긴 제목이지만 흔히 Pix2Pix로 알려진 아키텍처를 제시하는 논문이다.  
CGAN을 기반으로 하여 입력에 Condition과 input image를 같이 넣어 translated 된 이미지를 출력한다. (L1 loss 사용)  

## Abstract  

![image](https://user-images.githubusercontent.com/93988405/211950992-783071ba-e83e-4a71-8d7a-2756de1303a0.png)  
Conditional adversarial networks를 기반으로 범용적인 image-to-image translation task를 수행하는 방법을 제기합니다.  
신경망은 <font color='red'>input에서 output으로 가는 mapping 뿐만 아니라, 이 mapping을 학습시키는 loss function도 학습</font>한다.  
이 접근이 label과 동기화, 경계선만 있는 이미지를 복원, 흑백이미지에 색 입히기 등등의 문제에 효과적임을 보였음.

## Introduction  

논문이 작성된 이유는 image-to-image translation의 일반적인 framework를 제시하는게 목적이다.  
이미 CNNs는 image prediction 문제를 해결하는데 많이 쓰이고 있다.  
CNNs는 loss function을 최소화하는 방향으로 학습해 좋은 결과를 내며 과정 또한 automatic 하지만 effective loss 설계에 많은 노력이 필요함.  
따라서  CNN에게 어떤 것을 최소화해야 할지 알려주어야 한다. 만약 예측값과 정답 사이의 유클리드 거리를 최소화 하라고만 하면 blurry한 이미지를 생성함(유클리드 거리는 그럴듯한 거리를 전부 평균 내어 최소화되기 때문)

그렇기에 high-level goal로 원하는걸 말해야 한다. 그부분에서는 그 목표에 맞게 loss를 줄여나가는 GAN이 있음.  
GAN은 가짜와 실제를 구분하지 못하게 학습 진행(데이터에 맞게 loss를 학습한다) -> 흐리지 않은 이미지 생성 가능.  
논문에서는 cGAN(conditional GAN) 이라는 조건부 생성모델을 사용. cGAN은 입력이미지에 조건을 같이 주어 해당되는 출력값을 주기에 image-to-image translation tasks에 적합하다.  

## Related work  

논문에서 나오는 개념들을 몇개 정리하였다.  

### - Structures losses for image modeling 

이미지 변환 문제는 per-pixel 분류 또는 회귀 문제로 다뤄졌다.  
이러한 공식의 output space는 **“unstructured”** 이며 각 결과 픽셀은 다른 픽셀에 독립적인 것처럼 다룬다.  
CGAN은 **“structured loss”** 를 학습하며 많은 논문들이 이러한 loss를 다룬다.  

### - Conditional GANs  

생성자와 판별자가 훈련하는 동안 추가 정보를 사용해 조건이 붙는 생성적 적대 신경망이다.
CGAN을 이용하면 우리가 원하는 class가 담긴 데이터를 생성할 수 있다.**(생성자와 판별자를 훈련하는 데 모두 label을 사용)**  
GAN은 noise vector를 받아들여 출력물을 만들어내는데 이는 **random noise vector의 값에 따라 무작위이다.**(사람이 통제할 수 없었음)  
But CGAN은 기존 GAN에 특정 조건(condition)을 주어 이를 통제하도록 함.  
<font color='red'>기존의 GAN에 condition(조건)값을 넣어주는 것으로 결과물을 조작할수 있게한다.</font>  
**(성능은 기존GAN에 비해 퀄리티가 떨어지지만 결과물 조작 가능하다는 메리트!!)**  

![image](https://user-images.githubusercontent.com/93988405/211953483-7e1abf31-538f-47ed-b827-b90a1b884eed.png)

### - U-Net  

![image](https://user-images.githubusercontent.com/93988405/211953827-77a22367-71af-46a4-bfa1-050978554fea.png)  

Biomedical 분야에서 이미지 분할(Image Segmentation)을 목적으로 제안된 End-to-End 방식의 Fully-Convolutional Network 기반 모델이다.  
FCN의 "skip architecture" 개념을 활용해 얕은 층의 특징맵을 깊은 층의 특징맵과 결합하는 방식을 제안  
이미지의 크기를 줄이면서도 이미지 내의 중요한 정보를 직접 전달(skip-connection)하여 디코더에서 선명한 이미지를 얻게 하여 보다 정확한 예측가능!!  

> End-to-End Learning
>   어떤 문제를 해결할 때 필요한 여러 스텝을 하나의 신경망을 통해 '재배치'하는 과정.  
>   데이터 크기가 클 때 효율적. 즉 데이터가 클 때 두 단계로 나누어 각각 네트워크를 구축한 후 학습한 후 그 결과를 합치는 방법이다.  
>   이렇게 하는 이유는 스텝을 나누는 것이 성능이 더 좋기 때문  

### - Patch GAN  

![image](https://user-images.githubusercontent.com/93988405/211954279-c4817da3-ccc0-447d-a3f1-a046ecf0d1a8.png)  

전체 영역이 아니라 특정 크기의 patch 단위로 Generator가 만든 이미지의 진위여부 판단한다.  
기존 GAN에서 Discriminator는 Generator가 만들어준 입력 데이터(이미지) 전부를 보고 Real/Fake 여부를 판단하는데  
따라서 Generator는 Discriminator를 속이기 위해 데이터의 일부 특징을 과장하려는 경향을 보이는데  
Generator는 사람이 보는 이미지의 퀄리티 여부와 상관없이 Discriminator를 잘 속이는 방향으로만 학습을 하게 되고, 이로 인해 결과 이미지가 blurred 된다.  
따라서 보통 전체 이미지에 대한 Low frequency 성분을 L-1 regularization term을 통해 파악한 후  
High frequency 성분을 잘 보는 PatchGAN D와 결합하는 식(summation)으로 Discriminator의 loss를 구성한다.  

**<사용 이유>**  

1. 전체 이미지가 아니라 작은 이미지 패치 단위에 대해 sliding window가 지나가며 연산을 수행하므로 파라미터 개수가 훨씬 작아지므로 전체 이미지 크기에 영향을 받지 않고 연산속도가 빨라짐. -> discriminator 네트워크 크기를 줄일수 있다.  

2. low frequency에 대해 학습하는 L1 regularization term과 local 영역에서 sharpen한 디테일, 즉 high frequency 영역(엣지)에 대해 patch 단위로 학습함으로서 두 방식의 장점을 모두 취할 수 있다.  

## Method  

구조는 GAN과 마찬가지로 Generator는 실제이미지와 구별이 안되는 이미지를 생성하려는 반면, Discriminator는 생성된 이미지를 fake로 판별한다.  
BUT GAN은 random noise vector z에서 output image y로의 mapping (G:y->z)을 학습하는 생성모델이지만 Conditional GAN은 조건으로 입력되는 이미지 x와 random noise vector z에서 y로의 mapping (G:x,y->z)을 학습.  

## Objective  

Objective of conditional GAN can be expressed as  

![image](https://user-images.githubusercontent.com/93988405/211956595-8d5325b3-05aa-4092-898d-f03f37dcff79.png)  

Generator 는 loss를 최소화하려는 반면, Discriminator는 loss를 최대화하려 한다.  

Discriminator를 조건부로 학습시키는 것의 중요성 테스트를 위해 unconditional variant를 비교하게함  

![image](https://user-images.githubusercontent.com/93988405/211956878-c16e3da3-50d2-49ba-9181-7ad277d3c705.png)  

GAN의 loss함수에 L2, L1과 같은 traditional loss를 섞는 것이 더 좋다.  
Discriminator의 역할은 변하지 않지만, Generator는 Discriminator를 속이는 것에 더하여 ground truth값이 traditional loss(L1)에 따라 가까워지도록 학습된다. **Generator는 Discriminator를 속이는 것뿐만 아니라 L1 의미에서의 ground truth에도 가깝도록 만들어야 한다.**  
본 논문에서는 L2 distance보다 L1을 사용하는 것이 less bluring하다고 하여, L1 loss를 아래식과 같이 추가해서 최종 목적함수는 맨 아래의 식과 같다.  

![image](https://user-images.githubusercontent.com/93988405/211957081-7453115e-4944-4826-a71b-d1320c644b97.png)  

L1을 사용하는 이유에 대해 조금 더 설명을 덧붙이자면 L1 loss를 사용하면 이미지의 low frequency 성분들을 잘 검출해낸다.  
이미지에서 frequency란 픽셀 변화의 정도! <font color='red'>사물 내에서는 색 변화가 크지 않아 low frequency, 사물의 경계에서는 색이 급격히 변하기 때문에 high freq 따라서 L2 loss를 사용하면 더 blurry하다.</font>  

L1 loss를 사용했을 때 blurry 하지만 low freq 성분들을 잘 검출해낸다. 따라서 이는 그대로 두고 Discriminator에서 high freq를 검출한다. 이 때 이미지 전체는 필요없기 때문에 local image patch 즉 patch GAN 사용.  

기존의 condtional GANs처럼 noise z를 단순히 generator의 input으로 같이 넣어주는 것은, generator가 학습 과정에서 노이즈를 무시하며 학습하기 때문에 효율적이지 않다. 따라서 논문에서는 noise를 dropout form의 형태로(train, test time에 dropout 적용), 여러 층에서 제공될 수 있도록 만들었다.
> GAN에서 noise는 종이의 역할을 한다? Generator는 noise로부터 진짜 이미지로 맵핑하는 것.  

## Network architectures  

논문 제목에서도 알 수 있듯이 pix2pix는 파라미터로부터 이미지를 생성하는게 아닌 이미지로부터 이미지를 생성하는 기술이다.  

### - Generators with skip  

이미지 변환(image-to-image translation) 문제에서 어려운 점은 고해상도 input grid를 고해상도 output grid로 mapping 하는 것. encoder-decoder네트워크에서는 bottleneck 레이어를 통과하기 때문에 정보의 손실이 발생. 따라서 skip-connection을 추가한 U-Net 구조를 사용.(전체 레이어 개수를 
n 이라 할 때 모든 i 번째 레이어와 n−i 번째 레이어를 연결했다. 각 연결은 단순한 연결) 이를 통해 ***low-level 정보들이 보존되어 input&output에 사용***  

### - Markovian discriminator = Patch GAN  

L2, L1 loss만을 사용하게 되면 blurry한 결과들이 생성되는 문제가 있다. 하지만 L1 loss는 low frequencies를 정확하게 capture한다는 장점이 있으므로 GAN discriminator는 high-frequency structure을 모델링하고, low-frequency는 L1에 맡겨두면 된다.  

High frequncies를 모델링하기 위해 논문에서는 local image patches 단위로만 제한하는 것만으로 충분하다고 하고 있으며<font color='red'>(디테일한 부분을 파악하는데에 전체 이미지는 필요 없으니깐)</font>, disciminator에서 patchGAN이라 불리는 구조를 사용했다.  
Disciminator는 N x N patch 크기로 patch가 real/fake를 판단하며, 모든 이미지 패치들에 대한 convolution을 통해서 output을 얻게 됩니다. Patch size N은 매우 작아도 high quality result를 만들 수 있고, 적은 파라미터수와 빠른 연산의 장점이 있다.  

### - Optimization and inference  

신경망을 최적화하기 위해서 논문에서는 D와 G를 번갈아가며 gradient descent step을 진행하였다. <=D를 학습시키는 동안 loss를 절반으로 나눠서, G에 학습속도에 맞게 하였다> 또한 minibatch SGD와 Adam 사용.  

## Experiments  

논문에서는 생성모델의 성능 지표로 AMT perceptual studies와 FCN-score을 사용한다.  

> AMT perceptual
>  AMT perceputal 지표는 실제 사람들에게 ground truth와 생성된 이미지를 보여주고 "real vs fake"를 고르도록 하여 성능을 측정하는 방식.

> FCN-score
>   FCN-score는 pre-trained semantic classfier(FCN-8s)를 이용하여 생성된 이미지들의 segmanctic segmentation 결과를 통해서 realistic 한지 판단하는 것.  
>   즉 generated된 photo들이 얼마나 interpretable한지 측정하기 위해서, segementation 알고리즘 기반의 fully-connected-network를 사용하여 얻은 label map과 ground-truth 이미지들을 standard한 semantic segmentation 알고리즘을 통해 얻은 label과의 비교를 통해서 측정하는 방법.  

### - Analysis of the objective function  

![image](https://user-images.githubusercontent.com/93988405/211958902-e55be2ec-370b-47f0-bf9a-3f36c5f9cbf7.png)  

loss를 L1 term, GAN term으로 분리하여 각각만 사용해본 경우와 비교해보고, input에 condition을 준 경우와 그렇지 않은 경우의 discriminator를 비교하였다.  
**L1 loss만을 사용하는 경우에는** 그럴듯 하지만, blurry한 결과를 얻는다. **cGAN loss만을 사용하는 경우에는** sharper한 result를 얻지만 뭔가 인공적인 느낌이 있다.  
**Condition을 주지 않는 경우에는** input-output 간의 mismatch를 penalize 하지 않고 출력이 realistic한것만 신경씀.  

Loss는 input과 output간의 mismatch를 측정하는 지표이며, cGAN이 GAN보다 좋은 결과를 낸다. 또한 L1 term을 추가함으로써, ground truth과 output의 distance를 줄여주며, input과 match 될 수 있도록 해준다. 또한 L1은 ground truth의 분포보다 narrow하게 만들고 cGAN을 사용함으로써 출력 분포를 실측값에 더 가깝게 푸시한다.  

### - Analysis of the generator architecture 

U-Net 아키텍쳐를 통해 low-level 정보들이 신경망 전체에 전달된다고 합니다. (Encoder-Decoder에 skip-connection을 더하면 U-Net구조)  

### - From PixelGANs to PatchGANs to ImageGANs  

![image](https://user-images.githubusercontent.com/93988405/211959334-c3b2cefe-484f-430a-b981-f9d850f9f4c6.png)  

이 실험에서는 patch size N을 1(pixel단위)부터 이미지 전체 크기까지 receptive field를 넓혀가며 비교한 결과를 제시한다.  

PixelGAN은 spatial(공간적) sharpness는 없었지만 결과의 colorfulness는 상승. 16 x 16 patchGAN은 sharpness하고 좋은 FCN-score를 얻지만 인공물이 생김. 70 x 70 patchGAN에서 인공물을 없애며 score도 비슷했다. 286 x 286은 좋지 않았는데 이는 파라미터도 많아지고 깊이도 깊어지기 때문에 학습이 더 어려웠기 때문일 것이다. -> **70 x 70 patch 사이즈가 가장 잘 된다.**  

![image](https://user-images.githubusercontent.com/93988405/211959701-4b3a22af-6813-4578-828a-1553c254c13a.png)  

patchGAN의 크기에 영향을 받지 않는다는 장점을 살려 더 큰 이미지로도 학습해봄  

### - Perceptual validation  

실제로 AMT experiment한 결과를 분석하였다.  

![image](https://user-images.githubusercontent.com/93988405/211959934-3eb3a862-10c0-46d1-abb6-04845306be30.png)  

사실 colorization 문제와 같은 것에서는 colorization에 특화된 네트워크가 더 좋은 결과를 내기는 한다. 그러나 이 Pix2Pix는 훨씬 더 넓은 범위의 문제를 커버할 수 있다는 점에서 의의가 있는것 같다.  

## Conclusion  

논문에서는 conditional adversarial networks를 사용한 범용적인 image-to-image translation 방법에 대해 제시하였다. 신경망은 task와 data에 맞게 loss를 학습하며 이를 통해 여러 setting이 가능하고 넓은 범위의 문제에 대해 적합하다!!  


## <정리>  

지금까지 논문을 읽고 여러 reference들과 구글링을 통해 개념을 정리하였다. 결국 Pix2Pix 과정?에 대해 간략하게 정리하자면 다음과 같다.  

논문이 쓰여지기 전까지 이미지를 이미지로 변환하는 image to image translation 모델들은 predict pixels from pixels이라는 동일한 setting임에도 불구하고 각각의 문제들에 있어 각각 따로 연구되었다. 따라서 변환 문제들을 위한 어떤 일반적인 framework를 개발하는 것을 목표로 논문이 작성됨.
but 이러한 방향에 이미 CNN(convolutional neural nets)이라는 좋은 모델이 있다. 대부분의 CNN은 결과의 품질을 알려주는 loss function을 최소화 하는 방향으로 학습됨 => 좋은 결과를 위해 수많은 manual effort 필요
즉 여전히 CNN에게 우리가 뭘 최소화하고 싶은지 알려주어야 한다!!  

우리가 CNN에게 예측값과 실제값 사이의 (유클리디안)거리를 줄여주길 원하다고 요구하면 output은 blurred되어서 나온다. 이는 generator가 새로운 이미지를 생성할 때 input만을 보고 정확하게 파악이 불가능해 어느것을 택해도 loss가 너무 커지지 않도록 중간의 애매한 것을 택하는 경향 즉 average를 학습하려는 경향 때문이다.(low freq부분에서는 효과적 but high freq를 못 살림)  

따라서 두 번째 요구로 생성된 이미지가 조금 더 실제 같기를 원한다 요구
여기서 바로 GAN이 등장. 그냥 GAN이 아닌 conditional GAN을 사용하는 이유는 어떤 이미지가 주어졌을 때 그 이미지를 우리가 원하는 방향으로 변환하고 싶기 때문이다. 그저 진짜 같은 새로운 이미지가 아닌 입력 이미지를 조건으로 삼아 입력 이미지와 연관된 이미지를 생성할 수 있게 된것!!!  

![image](https://user-images.githubusercontent.com/93988405/211960519-114a092a-e296-4e56-b3bf-6002640205cc.png)  

cGAN Loss 면 충분해보이는데 왜 굳이 L1 loss를 합쳤는가?
이유는 바로 아무리 cGAN이 input x를 참고하더라도 generator G의 궁극적인 관심사는 오직 D를 속이는 것이기 때문에 x가 상대적으로 덜 반영될 수 있기 때문이다. 따라서 만들어진 이미지 G(x,z)와 대응되는 y 이미지를 직접 비교하는 L1 Loss를 추가할 필요가 있다.

## Reference  

- Image to Image Translation with Conditional Adversarial Networks  CVPR_2017
- https://di-bigdata-study.tistory.com/8
- greeksharifa.github.io/generative%20model/2019/04/07/Pix2Pix/
- https://velog.io/@wilko97/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Image-to-Image-Translation-with-Conditional-Adversarial-Networks-2017-CVPR
