---
layout: post
title:  "Image-to-Image Translation with Conditional Adversarial Networks 논문 공부"
---

# Image to Image Translation with Conditional Adversarial Networks  

긴 제목이지만 흔히 Pix2Pix로 알려진 아키텍처를 제시하는 논문이다.  
CGAN을 기반으로 하여 입력에 Condition과 input image를 같이 넣어 translated 된 이미지를 출력한다. (L1 loss 사용)  

## Abstract  

![image](https://user-images.githubusercontent.com/93988405/211950992-783071ba-e83e-4a71-8d7a-2756de1303a0.png)  
Conditional adversarial networks를 기반으로 범용적인 image-to-image translation task를 수행하는 방법을 제기합니다.  
신경망은 <span style="color:red">input에서 output으로 가는 mapping 뿐만 아니라, 이 mapping을 학습시키는 loss function도 학습</span>한다.  
이 접근이 label과 동기화, 경계선만 있는 이미지를 복원, 흑백이미지에 색 입히기 등등의 문제에 효과적임을 보였음.

## Introduction  

논문이 작성된 이유는 image-to-image translation의 일반적인 framework를 제시하는게 목적이다.  
이미 CNNs는 image prediction 문제를 해결하는데 많이 쓰이고 있다.  
CNNs는 loss function을 최소화하는 방향으로 학습해 좋은 결과를 내며 과정 또한 automatic 하지만 effective loss 설계에 많은 노력이 필요함.  
따라서  CNN에게 어떤 것을 최소화해야 할지 알려주어야 한다. 만약 예측값과 정답 사이의 유클리드 거리를 최소화 하라고만 하면 blurry한 이미지를 생성함(유클리드 거리는 그럴듯한 거리를 전부 평균 내어 최소화되기 때문)

그렇기에 high-level goal로 원하는걸 말해야 한다. 그부분에서는 그 목표에 맞게 loss를 줄여나가는 GAN이 있음.  
GAN은 가짜와 실제를 구분하지 못하게 학습 진행(데이터에 맞게 loss를 학습한다) -> 흐리지 않은 이미지 생성 가능.  
논문에서는 cGAN(conditional GAN) 이라는 조건부 생성모델을 사용. cGAN은 입력이미지에 조건을 같이 주어 해당되는 출력값을 주기에 image-to-image translation tasks에 적합하다.  

## Related work  

논문에서 나오는 개념들을 몇개 정리하였다.  

### - Structures losses for image modeling 

이미지 변환 문제는 per-pixel 분류 또는 회귀 문제로 다뤄졌다.  
이러한 공식의 output space는 **“unstructured”** 이며 각 결과 픽셀은 다른 픽셀에 독립적인 것처럼 다룬다.  
CGAN은 **“structured loss”** 를 학습하며 많은 논문들이 이러한 loss를 다룬다.  

### - Conditional GANs  

생성자와 판별자가 훈련하는 동안 추가 정보를 사용해 조건이 붙는 생성적 적대 신경망이다.
CGAN을 이용하면 우리가 원하는 class가 담긴 데이터를 생성할 수 있다.**(생성자와 판별자를 훈련하는 데 모두 label을 사용)**  
GAN은 noise vector를 받아들여 출력물을 만들어내는데 이는 **random noise vector의 값에 따라 무작위이다.**(사람이 통제할 수 없었음)  
But CGAN은 기존 GAN에 특정 조건(condition)을 주어 이를 통제하도록 함.  
<span style="color:red">기존의 GAN에 condition(조건)값을 넣어주는 것으로 결과물을 조작할수 있게한다.</span>  
**(성능은 기존GAN에 비해 퀄리티가 떨어지지만 결과물 조작 가능하다는 메리트!!)**  
![image](https://user-images.githubusercontent.com/93988405/211953483-7e1abf31-538f-47ed-b827-b90a1b884eed.png)

### - U-Net  

![image](https://user-images.githubusercontent.com/93988405/211953827-77a22367-71af-46a4-bfa1-050978554fea.png)  

Biomedical 분야에서 이미지 분할(Image Segmentation)을 목적으로 제안된 End-to-End 방식의 Fully-Convolutional Network 기반 모델이다.  
FCN의 "skip architecture" 개념을 활용해 얕은 층의 특징맵을 깊은 층의 특징맵과 결합하는 방식을 제안  
이미지의 크기를 줄이면서도 이미지 내의 중요한 정보를 직접 전달(skip-connection)하여 디코더에서 선명한 이미지를 얻게 하여 보다 정확한 예측가능!!  

> End-to-End Learning
>   어떤 문제를 해결할 때 필요한 여러 스텝을 하나의 신경망을 통해 '재배치'하는 과정.  
>   데이터 크기가 클 때 효율적. 즉 데이터가 클 때 두 단계로 나누어 각각 네트워크를 구축한 후 학습한 후 그 결과를 합치는 방법이다.  
>   이렇게 하는 이유는 스텝을 나누는 것이 성능이 더 좋기 때문  

### - Patch GAN  

![image](https://user-images.githubusercontent.com/93988405/211954279-c4817da3-ccc0-447d-a3f1-a046ecf0d1a8.png)  

전체 영역이 아니라 특정 크기의 patch 단위로 Generator가 만든 이미지의 진위여부 판단한다.  
기존 GAN에서 Discriminator는 Generator가 만들어준 입력 데이터(이미지) 전부를 보고 Real/Fake 여부를 판단하는데  
따라서 Generator는 Discriminator를 속이기 위해 데이터의 일부 특징을 과장하려는 경향을 보이는데  
Generator는 사람이 보는 이미지의 퀄리티 여부와 상관없이 Discriminator를 잘 속이는 방향으로만 학습을 하게 되고, 이로 인해 결과 이미지가 blurred 된다.  
따라서 보통 전체 이미지에 대한 Low frequency 성분을 L-1 regularization term을 통해 파악한 후  
High frequency 성분을 잘 보는 PatchGAN D와 결합하는 식(summation)으로 Discriminator의 loss를 구성한다.  

**<사용 이유>**  

1. 전체 이미지가 아니라 작은 이미지 패치 단위에 대해 sliding window가 지나가며 연산을 수행하므로 파라미터 개수가 훨씬 작아지므로 전체 이미지 크기에 영향을 받지 않고 연산속도가 빨라짐. -> discriminator 네트워크 크기를 줄일수 있다.  

2. low frequency에 대해 학습하는 L1 regularization term과 local 영역에서 sharpen한 디테일, 즉 high frequency 영역(엣지)에 대해 patch 단위로 학습함으로서 두 방식의 장점을 모두 취할 수 있다.  

## Method  

구조는 GAN과 마찬가지로 Generator는 실제이미지와 구별이 안되는 이미지를 생성하려는 반면, Discriminator는 생성된 이미지를 fake로 판별한다.  
BUT GAN은 random noise vector z에서 output image y로의 mapping (G:y->z)을 학습하는 생성모델이지만 Conditional GAN은 조건으로 입력되는 이미지 x와 random noise vector z에서 y로의 mapping (G:x,y->z)을 학습.  

## Objective  





