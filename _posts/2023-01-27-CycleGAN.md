---
layout: post
title:  "Unpaired Image-to-Image Translation using Cycle Consistent Adversarial Networks(CycleGAN) 논문공부"
categories:
  - deeplearning
---

# Unpaired Image-to-Image Translation using Cycle Consistent Adversarial Networks  
긴 제목이지만 흔히 CycleGAN으로 알려진 아키텍처를 제시하는 논문이다.  

# < Abstract >  

![image](https://user-images.githubusercontent.com/93988405/215060480-fc569323-3d06-43cc-950a-84aa7a8cece1.png)  

image-to-image Translation은 보통 pair-image를 이용해 학습하지만 같은 이미지에 대해 2개의 특성을 갖는 이미지 쌍은 구하기 쉽지 않다. 
따라서 논문에서는 Pair-image를 사용하지 않고 단지 X 도메인 데이터셋과 Y 도메인 데이터셋을 이용해 두 도메인 간의 이미지를 변환하는 법을 학습시킨다. 
도메인 X를 도메인 Y로 변환하는 함수 G 외에도 도메인 Y를 도메인 X로 변환하는 역방향 함수 F도 학습하며, 두 과정을 거쳤을 때 원본 이미지가 다시 복원될 수 있도록 
cycle consistency loss를 사용해 학습한다.  

# < Introduction >  

![image](https://user-images.githubusercontent.com/93988405/215060744-6cf14909-2115-4666-b089-e0f1d9e6a1e7.png)  

논문에서는 paired training example 없이 이미지 집합의 고유한 특징들을 파악하고, 이 특성들이 어떻게 다른 이미지 집합으로 전이될 수 있는지에 대한 방법을 제시한다.
domain X로부터 domain Y로의 mapping을 G라고 하고, G에 대한 network를 학습시켰을 때, 생성된 G(X)는 Y와 구별할 수 없어야 한다. 이러한 관계는 GAN의 기본 원리로, 
generator에 의해 생성된 데이터가 target class 데이터의 분포와 가까워지면서, 최종적으로는 그럴싸한 target class가 되는 원리이다.
그러나 일반적인 GAN의 원리로는 본 연구에서 해결하려는 unpaired image-to-image translation의 mode collapse를 해결할 수 없다고 말한다.  

그래서 논문에서는 주기 일관성 특성을 이용하기로 한다.
<font color='red'>즉, X를 Y로 매핑하는 Generator G뿐만아니라 Y를X로 매핑하는 Generator F도 사용해 유의미하게 매핑 되는 Generator를 만드는 것.</font>
**G와 F는 서로의 역함수이며 둘 다 일대일 대응이여야 한다.**  

### Mode Collapse   

![image](https://user-images.githubusercontent.com/93988405/215062609-04d2c681-78ed-42b4-bd6c-8409ecf3773c.png)   


mode는 최빈값, 즉 가장 빈도가 높은 값을 말한다. Mode collapse라는 문제를 설명하자면, 그림과 같이 파란색의 실제 데이터의 분포가 주어졌을 때, 
우리는 generator가 이 실제 데이터의 분포와 최대한 유사하게 학습하기를 바라게 된다. 
그러나 단순히 loss를 줄이기 위해서 학습을 하기 때문에 G가 이렇게 전체 데이터 분포를 찾지 못하고 오른쪽 그림과 같이 하나의 mode에만 몰리게 되는 경우가 발생한다.
이렇게 되면 서로 다른 두 그림의 아웃풋이 동일한 사진이 나오게 된다. MNIST 데이터세트를 이용하여 GAN을 학습시키다 보면 같은 숫자만 계속해서 생성되는 현상을 볼 수 있는데, 
이것이 바로 mode collapsing이 발생한 것 <mark style='background-color: #dcffe4'>(생성자가 학습 데이터 전체의 분포를 학습하지 못하고 그 중 일부분만 배우게 되는 것)</mark>   

이러한 mode collapse 문제를 해결하기 위해 cycle consistent를 도입하게 되는데, G(X)와 Y를 구분하기 어렵도록 adversarial loss term을 사용하며,  
mapping network가 추가적으로 cycle consistent를 만족시키도록 cycle consistency loss term을 결합하여 사용한다. 즉 학습은 아래의 3개의 구조로 이루어져 있다.  

1. G : x -> y의 Adversarial loss (GAN)
2. F : y -> x의 Adversarial loss (역방향 학습)
3. x ≈ F(G(x))의 Cycle consistence loss
   
   
# < Related work >  

- Generative Adversarial Networks : GAN의 핵심 아이디어는, generated image들이 real image와 구분하지 못하도록 만드는 adversarial loss에 있다. 논문에서는, translated image들이 target domain에 있는 image들과 구분되지 않도록 adversarial loss를 사용.

- Image to Image Translation : 기존의 image to image translation은 paired dataset을 사용했지만 논문에서는 unpaired dataset을 사용.

- Cycle Consistency : Structured data를 regularize하기 위해 transitivity를 사용하는 방법은 예전부터 사용되었다. 본 연구에서는, domain X에서 Y로의 mapping인 G와 domain Y에서 X로의 mapping인 F를 비슷하게 만드는 similar loss를 소개한다.

- Neural Style Transfer : 논문에서 발표하는 이 방법은 painting에서 photo로 변환시키는 등, 다양한 분야로 확장시킬 수 있을 것이다.  

# < Formulation >  

본 연구의 목표는 두 domain X,Y를 연결하는 mapping function을 학습시키는 것이다. 위 그림의 (a)처럼, 저자들이 제안하는 model은 두 개의 mapping을 갖고 있다. (G:X→Y, F:Y→X)

또한, 두 개의 adversarial discriminator를 가진다. Dx 는 실제 domain X의 이미지 x와 F가 생성한 F(y)를 구분하기 위한 것이고 Dy 는 y와 G(x)를 구분하기 위한 것.  

### 1. Adversarial Loss   

![image](https://user-images.githubusercontent.com/93988405/215062983-4c801780-4a04-45d3-a1be-326ec7ae42c8.png)   

![image](https://user-images.githubusercontent.com/93988405/215063019-1314778c-df68-4971-8270-f6c7c143be1e.png)
   

mapping function 두 개에 모두 adversarial loss를 적용함. Generator와 Discriminator가 각각 2개로 adversarial loss도 2개가 된다.

앞에서 했던 GAN의 loss와 동일하다. Discriminator의 목표는 진짜인지 가짜인지 잘 구별하는 것이고 Generator의 목표는 가짜를 진짜같이 만드는 것이다.

우선 왼쪽항은 G,DY,X,Y로 구성된 함수 LGAN이 있을 때 G는 LGAN(loss)의 값을 낮추려하고 DY는 LGAN의 값을 크게 하려 한다.
오른쪽항을 보시면 우선 Generator는 두번째항에만 관여합니다.
Generator가 목표대로 잘 작동한다면 Discriminator가 진짜라고 속아 output이 1에 가깝게 나와 항이 작아져 LGAN값이 작아질 것입니다.
반대로 Discriminator가 목표대로 잘 작동한다면 Discriminator에 Y를 넣으면 Discriminator의 output이 1에 수렴하고 Y^을 넣었을 때는 0에 수렴해 LGAN값이 최대로 높일 것입니다.   

### 2.Cycle Consistency Loss   

![image](https://user-images.githubusercontent.com/93988405/215063182-411acac9-2f1e-4da9-b07b-bb36c6fd6d70.png)   


Adversarial training은 이론적으로 타겟 do mains Y와 X로 동일하게 분포된 출력을 생성하는 매핑 G와 F를 학습한다. 하지만, network는 하나의 image를 target domain에 있는 여러 image와 mapping할 수 있다. 이는, adversarial loss 하나로 individual input  가 우리가 원하는 output  와 mapping 될 수 있다는 보장이 없다는 뜻. 그렇기에, mapping function을 cycle consistent하게 했다.

위 그림의 (b)를 보면, x→G(x)→F(G(x))≈x인 것을 확인할 수 있는데, 이를 forward cycle consistency라고 부른다. ((c)는 backward cycle consistency)
cycle consistency loss를 위해, 위와 같은 두개의 consistency를 만들고, 이를 loss에 반영했다. 해당 loss를 통해 unpaired domain X, Y를 쌍으로 연결할 수 있게 된다.(L1 loss를 adversarial loss로 바꿔도 봤지만 향상되지 않음)   

### 3.Full Objective   

![image](https://user-images.githubusercontent.com/93988405/215063396-cad82327-c127-4602-b2c7-6e36028e4062.png)   

따라서 최종 loss는 앞서 언급한 Adversarial loss와 Cycle Consistency loss를 결합한 형태로 식과 같이 표현된다. 해당 loss를 통해 generator는 실제 이미지에 가까운 이미지를 만들기 위해 노력하며, discriminator는 generated 된 이미지를 실제 이미지와 판별하는 것을 목표로 한다. 이 때 consistency loss 앞에 붙는 가중치 λ는 GAN losses와의 상대적 중요도에 따라 결정된다.

이 모델은 두 개의 autoencoders를 학습히키는 것처럼 볼 수 있는데, 이 autoencoder는 각각 특별한 내부 구조를 갖고 있어 다른 도메인에 image를 translation하는 것으로 자기 자신에 맵핑한다? 이러한 구조는 adversarial autoencoders의 특이케이스로 볼 수 있다.   

# < Implementation >   

CycleGAN은 Perceptual losses for real-time style transfer and super-resolution의 아키텍처를 기반으로 한다.   

### 1. Generator Architecture   

![image](https://user-images.githubusercontent.com/93988405/215063756-1d6e1bc7-8c76-45c5-be24-1d54a5f3c700.png)   

CycleGAN의 generator는 encoder, transformer, decoder 3개의 섹션으로 구성되어 있다. input image가 인코더로 들어가며 사이즈는 줄고 채널수는 증가한다 이제 transformer를 거쳐 디코더에서 다시 사이즈가 증가한다. 각 계층 뒤에는 인스턴스 정규화 및 ReLU 계층이 있지만 단순성을 위해 생략되었다.
<mark style='background-color: #dcffe4'>(그림과같이 2개의 stride-2 convolution과 여러개의 residual blocks, 2개의 1/2-stride convolution으로 이루어져 있다.)</mark>   

### 2. Discriminator Architecture   

![image](https://user-images.githubusercontent.com/93988405/215064016-74159d62-5ab1-4f60-90ac-6fc713e39b6c.png)   

Discriminator network는 pix2pix와 동일하게 PatchGAN 70X70을 사용했다.   

### 3. Training details   

![image](https://user-images.githubusercontent.com/93988405/215064142-59eb7e58-d878-4409-a1dd-bed81511e69e.png)   

1. 학습을 안정화시키기 위해, 본 연구에서는 original GAN의 negative log likelihood 대신 Least Square loss를 사용했다. (훈련이 더 안정적이고, 좋은 결과를 준다고 함)

2. model oscillation을 줄이기 위해, Shrivastava et al's strategy를 활용했다.

3. G가 생성한 하나의 최근 image를 이용하여 D를 바로 훈련시키는 것이 아니라, 그동안 생성한 image history buffer를 활용하여 최근 생성한 50개의 image를 지속적으로(따로) 저장하고 그것들을 이용해 training을 진행했다.
<생성된 이미지 중 가장 최근의 50개를 따로 저장해 Discriminator가 이를 한꺼번에 분류(모델 진동 최소화)>

진짜 데이터 샘플과 거리가 먼 가짜 데이터 샘플이 있을 때, NLL Loss를 사용한다면  Generator의 입장에서는 이미 Discriminator를 잘 속이고 있기 때문에 학습할 필요가 없다. 즉 Vanishing Gradient가 일어나기 때문에, Discriminator를 잘 속인다는 이유만으로, 안 좋은 샘플을 생성하는 것에 대해 패널티를 줄 수가 없게 됩니다. 이 때, LS GAN을 사용한다면 실제 데이터 분포와 가짜 데이터 샘플이 거리가 먼 것에 대해서도 패널티를 줄 수 있으므로 Least Square loss를 사용한다.   

# < Result >   

![image](https://user-images.githubusercontent.com/93988405/215064958-9a75bade-dd5e-4451-9495-5c064f918951.png)   

Table 1을 보면, AMT에서 cycleGAN은 각각 약 25%의 사람들을 속인 것을 확인할 수 있다. 그러나, Table 2, 3에서 pix2pix가 cycleGAN보다 더 우수한 성능을 보이는 것을 확인할 수 있다. baseline model(CoGAN, BiGAN/ALL, SimGAM, Feature loss+GAN)보다는 cycleGAN이 우수한 성능을 보이지만, paired data가 존재하는 경우에는 pix2pix를 사용하는 것이 더 효과적이지만 종종 지도학습인 pix2pix와 유사한 품질로 변환이 되었다는게 중요한 것 같다.   

> - **CoGAN**  
> GAN의 기본적인 Discriminator-Generator 구조를 따르고 있으며, 각 Domain마다 Generator와 Discrimator의 pair를 설정. 대응하는 이미지들이 high-level concept을 공유한다는 가설을 바탕으로 Generator와 Discriminator에서 각각 high level을 담당하는 부분의 weight을 공유한다.  


# < Ablation Study >  

> - **ablation study**  
> 모델이나 알고리즘을 구현하는 다양한 구성 요소중 어떠한 feature를 제거할 때 성능에 어떠한 영향을 미치는지 파악하는 방법.  

![image](https://user-images.githubusercontent.com/93988405/215065492-56659131-290d-4031-8229-764a6bda6d77.png)   

각각의 loss에 대한 영향력을 확인하기 위한 실험이다. figure 6을 보면 cycle alone(주기 일관성 손실)과 GAN alone(적대적 손실)만 사용했을 때, 그리고 각각 한 방향의 적대적 손실을 쓸 때 그리고 마지막이 논문에서 제안한 주기 일관성 손실과 적대적 손실을 다 사용했을 때이다. 적대적 손실이랑 주기 일관성 각각만 사용했을 때 결과가 크게 저하된 것을 볼 수 있는데 이로써 두 개의 loss 모두가 결과에 중요하다는 것을 알 수 있습니다.
또한 주기 일관성 손실을 하나씩만 쓰면 종종 훈련 불안정성을 야기하고 모드 붕괴를 유발하며, 특히 제거된 매핑 방향에 대해 모드 붕괴를 유발한다고 한다.   

# < Applications >   

![image](https://user-images.githubusercontent.com/93988405/215065793-57a4dae8-1043-40de-b522-34598393762d.png)   

figure 7 ~ 8을 보면 사진을 여러 화풍으로 바꾸거나 그림을 사진처럼 바꾸기도 한다. 또한 얼룩말과 말과의 변환, 겨울과 여름과의 변환, 오렌지와 사과의 변환등의 많은 응용이 가능하다.   

# < Limitations >   

![image](https://user-images.githubusercontent.com/93988405/215065935-0892156d-0b83-4e60-a799-3d6deff6755f.png)   

결과가 균일하게 잘 나오지 않는다. color, texture 변경을 포함하는 변환작업에서는 종종 성공하지만, 고양이를 개로 변환하는 작업 같은 모양 자체를 바꾸는 작업은 거의 성공하지 못했다. (저자는 이러한 기하학적 변화를 다루는 것은 앞으로의 풀어 가야할 중요한 문제라고 언급.)

training dataset의 특성 분포에서 야기되는 문제도 있다. 예를 들어 설명하면 말을 얼룩말로 변환하는 모델을 학습시킬 때 training dataset에 사람이 들어가지 않고 학습하게 되면 사람이 포함되어 있는 사진으로 test를 한다면 사람과 말을 구분하지 못하고 모두 얼룩말의 패턴으로 변환된다.

마지막으로는 paired dataset으로 훈련 시킨 것과 아닌 것의 사이의 차이가 여전하다는 것이다. 어떤 경우에는 이 차이를 좁히기 매우 어렵거나 심지어 불가능할 수도 있다.   

## Reference   

- papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper  
- https://velog.io/@lilpark/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Unpaired-Image-to-Image-Translation-using-Cycle-Consistent-Adversarial-Networks  
- https://velog.io/@victory/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0Unpaired-Image-to-Image-Translation-using-Cycle-Consistent-Adversarial-NetworksCycleGAN
