---
layout: post
title:  "Pix2Pix ì½”ë“œ ì‹¤ìŠµê³¼ ì„¤ëª…!!!"
---

```python
from os import listdir
from os.path import join
import random
import matplotlib.pyplot as plt
%matplotlib inline

import torch
import os
import time
from PIL import Image
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision.transforms.functional import to_pil_image

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```


```python
!git clone https://github.com/mrzhu-cool/pix2pix-pytorch # ê¹ƒ í´ë¡ 
```

    Cloning into 'pix2pix-pytorch'...
    remote: Enumerating objects: 68, done.[K
    remote: Counting objects: 100% (23/23), done.[K
    remote: Compressing objects: 100% (9/9), done.[K
    remote: Total 68 (delta 17), reused 14 (delta 14), pack-reused 45[K
    Unpacking objects: 100% (68/68), 84.94 MiB | 12.21 MiB/s, done.
    


```python
!mkdir 'data' # í´ë” ìƒì„±
```


```python
!unzip /content/pix2pix-pytorch/dataset/facades.zip  -d /content/data; # ì••ì¶• í’€ê¸°
```
    

# < PIx2PIx >  

pix2pixì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ë§í•˜ìë©´, ì™¼ìª½ì‚¬ì§„ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ì¡°ê±´ ì´ë¯¸ì§€ ì¸ë°, ì´ëŸ¬í•œ ì´ë¯¸ì§€ê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì™”ì„ ë•Œ ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ ì‚¬ì§„ìœ¼ë¡œ ì°ì€ê±° ê°™ì€ ì´ë¯¸ì§€ê°€ ì¶œë ¥ë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒ.

ì´ë¯¸ì§€ëŠ” ìœ„ì—ì„œ ë³¸ê²ƒì²˜ëŸ¼ ì…ë ¥ê³¼ ì¶œë ¥í˜•íƒœì˜ ì´ë¯¸ì§€ê°€ ë‘ ê°œê°€ ë¶™ì–´ìˆëŠ” í˜•íƒœì´ì§€ë§Œ, í•™ìŠµì„ ì§„í–‰í• ë•ŒëŠ” ë”°ë¡œë”°ë¡œ ë´ì•¼ë˜ë¯€ë¡œ ì´ë¯¸ì§€ë¥¼ ë„¤íŠ¸ì›Œí¬ì— ë„£ê¸° ìœ„í•´ custom datasetì„ ì •ì˜í•´ì¤€ë‹¤.  

> <Dataset & Dataloader>
> 
> Dataset ì€ ìƒ˜í”Œê³¼ ì •ë‹µ(label)ì„ ì €ì¥í•˜ê³ , DataLoader ëŠ” Dataset ì„ ìƒ˜í”Œì— ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ê°ì²´(iterable)ë¡œ ê°ìŒ‰ë‹ˆë‹¤.
>
> - Dataset : ì „ì²´ datasetì„ êµ¬ì„±í•˜ëŠ” ë‹¨ê³„ë¡œ dataloaderë¥¼ í†µí•´ dataë¥¼ ë°›ì•„ì˜¤ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. 
>
>> <Dataset classì—ì„œ ë°˜ë“œì‹œ ì •ì˜í•´ì•¼ í•˜ëŠ” method 3ê°€ì§€>
>>* init(self) : í•„ìš”í•œ ë³€ìˆ˜ ì„ ì–¸
>>* get_item(self,index) : ë§Œë“  ë¦¬ìŠ¤íŠ¸ì˜ indexì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œì„ ë°ì´í„°ì…‹ì—ì„œ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬í•˜ì—¬ tensor(ë°°ì—´, í–‰ë ¬) ìë£Œí˜•ìœ¼ë¡œ ë°”ê¿”ì„œ ë¦¬í„´
>>* len(self) : í•™ìŠµ ë°ì´í„° ê°œìˆ˜ ë¦¬í„´  
>
> - Dataloader : datasetìœ¼ë¡œë¶€í„° dataloaderë¥¼ ìƒì„±í•œë‹¤. DataloaderëŠ” Datasetì„ batch ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ë¯¸ë‹ˆë°°ì¹˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ê¸°ëŠ¥ì„ í•˜ëŠ”ë°, Dataloaderë¥¼ í†µí•´ Datasetì˜ ì „ì²´ ë°ì´í„°ê°€ batch sizeë¡œ ë‚˜ëˆ„ì–´ì ¸ ê³µê¸‰ë©ë‹ˆë‹¤.  


ì½”ë“œë¥¼ ë³´ë©´ ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìƒ‰ìƒ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ê³ , ì „ì¥ì—ì„œ ë´¤ë˜ ì‚¬ì§„ì„ aë¥¼ ì…ë ¥í˜•íƒœì˜ ì´ë¯¸ì§€, bë¥¼ ì¶œë ¥ í˜•íƒœì˜ ì´ë¯¸ì§€ë¡œ ì§€ì •í•´ë†“ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.


```python
# Custom dataset ìƒì„±

class FacadeDataset(Dataset):
    def __init__(self, path2img, direction='b2a', transform=False):
        super().__init__()
        self.direction = direction
        self.path2a = join(path2img, 'a')
        self.path2b = join(path2img, 'b')
        self.img_filenames = [x for x in listdir(self.path2a)]
        self.transform = transform

    def __getitem__(self, index):
        a = Image.open(join(self.path2a, self.img_filenames[index])).convert('RGB')
        b = Image.open(join(self.path2b, self.img_filenames[index])).convert('RGB')
        
        if self.transform:
            a = self.transform(a)
            b = self.transform(b)

        if self.direction == 'b2a':
            return b,a
        else:
            return a,b

    def __len__(self):
        return len(self.img_filenames)
     
```

# < Transform >  

pix2pix ëª¨ë¸ì˜ ê²½ìš° 256 x 256 ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— resize í•´ì¤˜ì•¼ í•œë‹¤. ì´ ë•Œ normalizeë¥¼ í•´ì£¼ëŠ”ë° ë²”ìœ„ë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ stepí•´ ë‚˜ê°€ëŠ” landscapeë¥¼ ì•ˆì •í™”ì‹œì¼œì„œ local optima ë¬¸ì œë¥¼ ì˜ˆë°©í•˜ê³ , ì†ë„ ì¸¡ë©´ì—ì„œë„ ì¢‹ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤.  

> - local optima : ì¤‘ê°„ì¤‘ê°„ ì›€í‘¹íŒŒì¸ë¶€ë¶„ì´ ìµœì†Œë¼ê³  ìƒê°ë˜ê²Œ ë§Œë“¤ì–´ì§€ëŠ” ë¬¸ì œ


```python
# transforms ì •ì˜
transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),
                    transforms.Resize((256,256))
])
     
```

### Normalize

resizeí•œ ì´ë¯¸ì§€ë“¤ì˜ meanê³¼ stdë¡œ normalizeë¥¼ ì§„í–‰í•´ì•¼ í•œë‹¤.(resizeí•œ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•  ê²ƒì´ê¸° ë•Œë¬¸ì— resizeí•œ ë°ì´í„°ì˜  í‰ê· (mean)ê³¼, í‘œì¤€í¸ì°¨(standard deviation)ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤.) but ê·¸ëƒ¥ meanê³¼ stdë¥¼ 0.5ë¡œ normalizationì„ ì§„í–‰í•˜ê¸°ë„ í•œë‹¤.


```python
# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
path2img = '/content/data/facades/train'
train_ds = FacadeDataset(path2img, transform=transform)
```


```python

```


```python
# ìƒ˜í”Œ ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°
a,b = train_ds[0]
plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.imshow(to_pil_image(0.5*a+0.5))
plt.axis('off')
plt.subplot(1,2,2)
plt.imshow(to_pil_image(0.5*b+0.5))
plt.axis('off')
```




    (-0.5, 255.5, 255.5, -0.5)




    
![image](https://user-images.githubusercontent.com/93988405/213350408-29082f9c-8d4d-4667-9235-ad908b177d68.png)

    


## Dataloader  

ì´ì œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ ì•ì—ì„œ ë§Œë“  custom datasetì— ë„£ì€ ê²ƒì„ train_dsì— ì €ì¥í•˜ê³  train_dsë¥¼ ì‚¬ìš©í•´ dataloderë¥¼ ë§Œë“¤ì–´ train_dlì— ì €ì¥í•œë‹¤.


```python
# ë°ì´í„° ë¡œë” ìƒì„±í•˜ê¸°
train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
```

# < ëª¨ë¸ êµ¬ì¶•í•˜ê¸° >

![image](https://user-images.githubusercontent.com/93988405/213350051-f48f36a3-d6b8-4dbc-9b41-033dd28d4a25.png)


pix2pixëŠ” U-netêµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤. U-netêµ¬ì¡°ë€ skip connectionì„ ì´ìš©í•´ ì™¼ìª½ ì‚¬ì§„ì˜ ì•„ë˜ìª½ í™”ì‚´í‘œì™€ ê°™ì´ ê¸°ë³¸ì ìœ¼ë¡œ ì¸ì½”ë” íŒŒíŠ¸ì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ ë””ì½”ë” íŒŒíŠ¸ì—ì„œ ì‚¬ìš©í•œë‹¤. ì¦‰ ì¸ì½”ë” íŒŒíŠ¸ì—ì„œ ì²˜ë¦¬í•˜ëŠ” low-level informationì´ ë””ì½”ë”© ë˜ëŠ” ì¶œë ¥ ê²°ê³¼ì—ì„œë„ ì¶©ë¶„íˆ í™œìš©ëœë‹¤.


```python
# U-Net Down
class UNetDown(nn.Module):
    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):
        super().__init__()

        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]

        if normalize:
            layers.append(nn.InstanceNorm2d(out_channels)),

        layers.append(nn.LeakyReLU(0.2))

        if dropout:
            layers.append(nn.Dropout(dropout))

        self.down = nn.Sequential(*layers)

    def forward(self, x):
        x = self.down(x)
        return x

# check
x = torch.randn(16, 3, 256,256, device=device)
model = UNetDown(3,64).to(device)
down_out = model(x)
print(down_out.shape)
```

    torch.Size([16, 64, 128, 128])
    

U-net ì•„í‚¤í…ì²˜ì˜ downsampling ëª¨ë“ˆì…ë‹ˆë‹¤. ì…ë ¥ê°’ì´ ë“¤ì–´ì˜¤ë©´ convolution layerë¥¼ ê±°ì¹ ìˆ˜ ìˆë„ë¡ í•˜ê³  kernel_size = 4, stride = 2, padding = 1 ë¡œ ì„¤ì •í•œë‹¤ -> ë„ˆë¹„ì™€ ë†’ì´ê°€ 2ë°°ì”© ê°ì†Œí•œë‹¤.(Channel sizeëŠ” ì¦ê°€í•˜ë„ë¡ ë§Œë“ ë‹¤)
ì´í›„ normalizeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ê³  leakyReluì™€ dropoutì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ê³  ì´ëŸ° ì „ì²´ layerë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ ëª¨ë¸ë¡œì¨ ì‚¬ìš©í•˜ëŠ”ë°
<font color='red'>ì½”ë“œì—ì„œ initë¶€ë¶„ì˜ ì „ì²´ layerë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ forward()í•¨ìˆ˜ì—ì„œ ëª¨ë¸ì´ í•™ìŠµë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ì„œ forward ì—°ì‚°ì„ ì§„í–‰ì‹œí‚¤ëŠ” êµ¬ì¡°ì´ë‹¤.</font>


```python
# U-Net Up
class UNetUp(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.0):
        super().__init__()

        layers = [
            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),
            nn.InstanceNorm2d(out_channels),
            nn.LeakyReLU()
        ]

        if dropout:
            layers.append(nn.Dropout(dropout))

        self.up = nn.Sequential(*layers)

    def forward(self,x,skip):
        x = self.up(x)
        x = torch.cat((x,skip),1)
        return x

# check
x = torch.randn(16, 128, 64, 64, device=device)
model = UNetUp(128,64).to(device)
out = model(x,down_out)
print(out.shape)
     
```

    torch.Size([16, 128, 128, 128])
    

upmodeling ì½”ë“œë¥¼ ë³´ë©´ skip connectionì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•ì˜ ì¸ì½”ë” íŒŒíŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ê°’ì„ ê°™ì´ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ **â€œdef forwardì—ì„œâ€**  skipìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ xê°€ ë“¤ì–´ì™”ì„ ë•Œ transposed convolution layerë¥¼ ì‚¬ìš©í•´ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ 2ë°°ì”© ì¦ê°€ì‹œí‚¨ë‹¤.(Channel sizeëŠ” ê°ì†Œí•˜ë„ë¡ ë§Œë“ ë‹¤) ê·¸ ì™¸ì˜ initì˜ layer êµ¬ì¡°ëŠ” downê³¼ ë™ì¼
def forwardë¥¼ ë³´ë©´ initì˜ layerë“¤ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë¬¶ì–´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ xì— ëŒ€í•´ì„œ ì´ëŸ¬í•œ ëª¨ë¸ì„ ê±°ì¹˜ë„ë¡ ë§Œë“¤ì–´ ì£¼ê³  ê·¸ ì¶œë ¥ì— íŠ¹ì •í•œ skipë¥¼ ë”í•´ì£¼ì–´ì„œ ê·¸ê²ƒì„ ìµœì¢… outputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
<font color='red'>ì´ë•Œ skipì„ ë”í•´ì£¼ëŠ” ê³¼ì •ì„ channel levelì—ì„œ í•©ì³ ì±„ë„ì„ ë‘ê»ê²Œ ë§Œë“ ë‹¤.</font>


```python
# generator: ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
class GeneratorUNet(nn.Module):
  #RGB colorì´ë¯€ë¡œ channel = 3
    def __init__(self, in_channels=3, out_channels=3):
        super().__init__()

        #down sampling ì´ë¯€ë¡œ ì±„ë„í¬ê¸°ëŠ” ì»¤ì§€ê³  ë„ˆë¹„ ë†’ì´ëŠ” ì ˆë°˜ì”©
        self.down1 = UNetDown(in_channels, 64, normalize=False) # ì¶œë ¥ : [64 x 128 x 128]
        self.down2 = UNetDown(64,128)                 # ì¶œë ¥ : [128 x 64 x 64]
        self.down3 = UNetDown(128,256)                # ì¶œë ¥ : [256 x 32 x 32]
        self.down4 = UNetDown(256,512,dropout=0.5)    # ì¶œë ¥ : [512 x 16 x 16]
        self.down5 = UNetDown(512,512,dropout=0.5)    # ì¶œë ¥ : [512 x 8 x 8]
        self.down6 = UNetDown(512,512,dropout=0.5)    # ì¶œë ¥ : [512 x 4 x 4]
        self.down7 = UNetDown(512,512,dropout=0.5)    # ì¶œë ¥ : [512 x 2 x 2]
        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5) # ì¶œë ¥ : [512 x 1 x 1]

        # skip-connectionì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì¶œë ¥ ì±„ë„ì˜ í¬ê¸° x 2 == ë‹¤ìŒ ì…ë ¥ì±„ë„ì˜ í¬ê¸°
        self.up1 = UNetUp(512,512,dropout=0.5)    # ì¶œë ¥ : [1024 x 2 x 2]
        self.up2 = UNetUp(1024,512,dropout=0.5)   # ì¶œë ¥ : [1024 x 4 x 4]
        self.up3 = UNetUp(1024,512,dropout=0.5)   # ì¶œë ¥ : [1024 x 8 x 8]
        self.up4 = UNetUp(1024,512,dropout=0.5)   # ì¶œë ¥ : [1024 x 16 x 16]
        self.up5 = UNetUp(1024,256)               # ì¶œë ¥ : [512 x 32 x 32]
        self.up6 = UNetUp(512,128)                # ì¶œë ¥ : [256 x 64 x 64]
        self.up7 = UNetUp(256,64)                 # ì¶œë ¥ : [128 x 128 x 128]
        self.up8 = nn.Sequential(
            nn.ConvTranspose2d(128,3,4,stride=2,padding=1),  # ì¶œë ¥ : [3 x 256 x 256]
            nn.Tanh()
        )

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)
        d7 = self.down7(d6)
        d8 = self.down8(d7) # ë³´í‹€ë„¥ ì •ì¤‘ì•™ì˜ ì ¤ ì‘ì€ ì •ì‚¬ê°í˜•

        u1 = self.up1(d8,d7) # ,ì˜¤ë¥¸ìª½ê°’ì€ skipê°’ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ë”í•´ì§€ëŠ”... ì¦‰ upsampling blockì— d8ì„ ë„£ì–´ ì²˜ë¦¬ëœ ê²°ê³¼ì— d7ì„ ê·¸ëŒ€ë¡œ ë”í•œë‹¤.
        u2 = self.up2(u1,d6)
        u3 = self.up3(u2,d5)
        u4 = self.up4(u3,d4)
        u5 = self.up5(u4,d3)
        u6 = self.up6(u5,d2)
        u7 = self.up7(u6,d1)
        u8 = self.up8(u7) # ê²°êµ­ ì…ë ¥ì´ë¯¸ì§€ì™€ ë™ì¼í•œ ì°¨ì›ì˜ ì¶œë ¥ì´ë¯¸ì§€ê°€ ëœë‹¤.

        return u8

# check
x = torch.randn(16,3,256,256,device=device)
model = GeneratorUNet().to(device)
out = model(x)
print(out.shape)
```

    torch.Size([16, 3, 256, 256])
    

ì´ì œ U-net down, up ëª¨ë“ˆì„ ì‚¬ìš©í•´ Generator ì•„í‚¤í…ì²˜ë¥¼ ë§Œë“¤ì—ˆë‹¤.
ì²˜ìŒ input channelì€ RGB colorì´ë¯€ë¡œ 3ì´ë˜ê³  down samplingì„ ì§„í–‰í•˜ì—¬ channel sizeëŠ” í‚¤ì›Œì¤€ë‹¤. ì£¼ì„ì— ì¨ë†“ì•˜ë“¯ì´ ì±„ë„ì˜ í¬ê¸°ëŠ” 2ë°°ì”© ì¦ê°€í•˜ê³  ë„ˆë¹„ì™€ ë†’ì´ëŠ” 2ë°°ì”© ê°ì†Œí•œë‹¤. ê·¸ë˜ì„œ ìµœì¢…ì ìœ¼ë¡œ 512 x 1 x 1í˜•íƒœê°€ ë˜ê³  ì´ì œ ë””ì½”ë” ë¶€ë¶„ì—ì„œ up samplingì„ ì§„í–‰í•œë‹¤. skip connectionì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ìŒ ì…ë ¥ ì±„ë„ì˜ í¬ê¸°ëŠ” (ì¶œë ¥ì±„ë„ì˜ í¬ê¸° x 2) ê°€ ëœë‹¤. ê·¸ë ‡ê²Œ ì­‰ layerë¥¼ ê±°ì¹˜ë©´ 3 x 256 x 256 í˜•íƒœ ì¦‰ ì…ë ¥ì´ë¯¸ì§€ì™€ ë™ì¼í•œ ì°¨ì›ì˜ ì¶œë ¥ì´ë¯¸ì§€ê°€ ë§Œë“¤ì–´ì§„ë‹¤. ì˜¤ë¥¸ìª½ì˜ forward ì—°ì‚° ë¶€ë¶„ì˜ up ë¶€ë¶„ì˜ ì²˜ìŒ ë¶€ë¶„ì„ ë³´ë©´ U-net êµ¬ì¡°ì˜ ì‚¬ì§„ì²˜ëŸ¼ upsampling blockì— d8ì„ ë„£ì–´ ì²˜ë¦¬ëœ ê²°ê³¼ì— d7ì„ ê·¸ëŒ€ë¡œ ë”í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•´ skip-connectionì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/93988405/213350104-bafd9d35-e32d-43ab-b1d7-b19003007813.png)


Pix2PixëŠ” conditional GANì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì§„ê³¼ ê°™ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì¡°ê±´ì´ë¯¸ì§€(x)ì™€ ê·¸ì—ë”°ë¥¸ y ë„ë©”ì¸ ì´ë¯¸ì§€(G(x))ê°€ ê°™ì´ ë“¤ì–´ì™€ì•¼ëœë‹¤. ë”°ë¼ì„œ í•­ìƒ discriminatorê°€ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ê²ƒì€ ì¡°ê±´+ê°€ì§œì´ë¯¸ì§€ í˜¹ì€ ì¡°ê±´ + ì§„ì§œ ì´ë¯¸ì§€ì´ë‹¤. 




```python
class Dis_block(nn.Module):
    def __init__(self, in_channels, out_channels, normalize=True):
        super().__init__()

        layers = [nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)]
        if normalize:
            layers.append(nn.InstanceNorm2d(out_channels))
        layers.append(nn.LeakyReLU(0.2))
    
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        x = self.block(x)
        return x

# check
x = torch.randn(16,64,128,128,device=device)
model = Dis_block(64,128).to(device)
out = model(x)
print(out.shape)
```

    torch.Size([16, 128, 64, 64])
    


```python
# Discriminatorì€ patch ganì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# Patch Gan: ì´ë¯¸ì§€ë¥¼ 16x16ì˜ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ê° íŒ¨ì¹˜ê°€ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ ì‹ë³„í•©ë‹ˆë‹¤.
# high-frequencyì—ì„œ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.

class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super().__init__()

        self.stage_1 = Dis_block(in_channels*2,64,normalize=False) # ì¶œë ¥ : [64 x 128 x 128]
        self.stage_2 = Dis_block(64,128)    # ì¶œë ¥ : [128 x 64 x 64]
        self.stage_3 = Dis_block(128,256)   # ì¶œë ¥ : [256 x 32 x 32]
        self.stage_4 = Dis_block(256,512)   # ì¶œë ¥ : [512 x 16 x 16]

        self.patch = nn.Conv2d(512,1,3,padding=1) # 16x16 íŒ¨ì¹˜ ìƒì„±

    def forward(self,a,b):
        x = torch.cat((a,b),1)
        x = self.stage_1(x)
        x = self.stage_2(x)
        x = self.stage_3(x)
        x = self.stage_4(x)
        x = self.patch(x)
        x = torch.sigmoid(x)
        return x
# check
x = torch.randn(16,3,256,256,device=device)
model = Discriminator().to(device)
out = model(x,x)
print(out.shape)
```

    torch.Size([16, 1, 16, 16])
    

DiscriminatorëŠ”
PatchGANì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì›í•˜ëŠ” patch ì‚¬ì´ì¦ˆì¸ 16x16ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ 4ê°œì˜ stageë¥¼ ì§€ë‚˜ë©° convolution ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ 16x16í˜•íƒœì˜ íŒ¨ì¹˜ë¥¼ ìƒì„±í•œë‹¤.
ì½”ë“œì˜ stage1ì„ ë³´ë©´ ì…ë ¥ ì´ë¯¸ì§€ê°€ 2ê°œì´ë‹ˆê¹ channel*2 í˜•íƒœì´ê³ , forward í•¨ìˆ˜ë¥¼ ë³´ë©´ ì…ë ¥ë°›ëŠ” ì´ë¯¸ì§€ê°€ 2ê°œì…ë‹ˆë‹¤. (ì…ë ¥ ì´ë¯¸ì§€ê°€ 2ê°œë¼ëŠ” ê²ƒì€ 2ê°œì˜ ì´ë¯¸ì§€ê°€ í•œìŒì„ ì´ë£¨ì–´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤ëŠ” ê²ƒ)
<font color='red'> stageë¥¼ ê±°ì¹ ìˆ˜ë¡ ì±„ë„ìˆ˜ëŠ” ëŠ˜ì–´ë‚˜ê³  ì´ë¯¸ì§€ì˜ ë†’ì´ì™€ ë„ˆë¹„ëŠ” ë°˜ì”© ì¤„ì–´ë“¤ê²Œë˜ì–´ stage4ì—ì„œëŠ” 512 x 16 x 16í˜•íƒœê°€ ë˜ê³  ì´ë¥¼ self.patchë¥¼ í†µí•´ 1 x 16 x 16ìœ¼ë¡œ(ì••ì¶•)ë§Œë“¤ì–´ì£¼ê²Œ ë˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ sigmoidë¥¼ ì·¨í•´ ê° í”½ì…€ê°’ì„ sigmoidí˜•íƒœë¡œ í•´ì¤ë‹ˆë‹¤.</font>


```python
model_gen = GeneratorUNet().to(device)
model_dis = Discriminator().to(device)
```


```python
# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
def initialize_weights(model):
    class_name = model.__class__.__name__
    if class_name.find('Conv') != -1:
        nn.init.normal_(model.weight.data, 0.0, 0.02)


# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš©
model_gen.apply(initialize_weights);
model_dis.apply(initialize_weights);
```

# < í•™ìŠµ >

- **BCEloss** : binary cross entropy loss(BCELoss)ë¡œ ì´ì§„ ë¶„ë¥˜ì— íŠ¹í™”ëë‹¤.
BCELossì—ì„œëŠ” CrossEntropyLossì™€ ê°™ì´ softmaxë¥¼ í¬í•¨í•œ ê²ƒì´ ì•„ë‹Œ, Cross Entropyë§Œ êµ¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ lossë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—” softmax ë˜ëŠ” ë‹¤ë¥¸ activation functionì„ ë”°ë¡œ ì ìš©í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

- **MSEloss** : Mean Squared Error lossë¡œ  image ê°„ì˜ ì°¨ì´ë‚˜ segmentation ì—ì„œëŠ” mask ê°„ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©


```python
# ì†ì‹¤í•¨ìˆ˜
#loss_func_gan = nn.BCELoss()
loss_func_gan = nn.MSELoss()
loss_func_pix = nn.L1Loss()

# loss_func_pix ê°€ì¤‘ì¹˜
lambda_pixel = 100

# patch ìˆ˜
patch = (1,256//2**4,256//2**4)

# ìµœì í™” íŒŒë¼ë¯¸í„°
from torch import optim
lr = 2e-4
beta1 = 0.5
beta2 = 0.999

opt_dis = optim.Adam(model_dis.parameters(),lr=lr,betas=(beta1,beta2))
opt_gen = optim.Adam(model_gen.parameters(),lr=lr,betas=(beta1,beta2))
```

ì‹¤ìŠµì„ í•  ë•Œ loss í•¨ìˆ˜ë¡œëŠ” MSElossë¥¼ ì‚¬ìš©í•˜ì˜€ê³  ë‚˜ì¤‘ì— generator lossì—ì„œ L1 lossì— ê³±í•  ê°€ì¤‘ì¹˜ì™€ ì—í¬í¬ë¥¼ 100ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. 


```python
# í•™ìŠµ
model_gen.train()
model_dis.train()

batch_count = 0
num_epochs = 100
start_time = time.time()

loss_hist = {'gen':[],
             'dis':[]}

for epoch in range(num_epochs):
    for a, b in train_dl:
        ba_si = a.size(0)

        # real image
        real_a = a.to(device) # ì…ë ¥ì´ë¯¸ì§€(ê·¸ë¦°ì´ë¯¸ì§€)
        real_b = b.to(device) # ì¶œë ¥ì´ë¯¸ì§€(ì‚¬ì§„)

        # patch label
        real_label = torch.ones(ba_si, *patch, requires_grad=False).to(device)
        fake_label = torch.zeros(ba_si, *patch, requires_grad=False).to(device)

        ### generator í•™ìŠµ ###
        model_gen.zero_grad()

        fake_b = model_gen(real_a) # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±
        out_dis = model_dis(fake_b, real_b) # ê°€ì§œ ì´ë¯¸ì§€ ì‹ë³„

        gen_loss = loss_func_gan(out_dis, real_label)
        pixel_loss = loss_func_pix(fake_b, real_b)  # ë§Œë“¤ì–´ì§„ ê²°ê³¼ê°€ ì‹¤ì œ ì‚¬ì§„ê³¼ ìœ ì‚¬í• ìˆ˜ ìˆë„ë¡(L1 lossë¡œ í•™ìŠµ)

        g_loss = gen_loss + lambda_pixel * pixel_loss
        g_loss.backward() # backpropagation -> lossë¥¼ ì—­ì „íŒŒí•œë‹¤(ì†ì‹¤ì˜ ë³€í™”ë„ ì €ì¥)
        opt_gen.step()    # .step()ì„ í˜¸ì¶œí•´ ì—­ì „íŒŒ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ë³€í™”ë„ë¡œ ë§¤ê°œë³€ìˆ˜ ì¡°ì •

        ### discriminator í•™ìŠµ ###
        model_dis.zero_grad()

        out_dis = model_dis(real_b, real_a) # ì§„ì§œ ì´ë¯¸ì§€ ì‹ë³„
        real_loss = loss_func_gan(out_dis,real_label)
        
        out_dis = model_dis(fake_b.detach(), real_a) # ê°€ì§œ ì´ë¯¸ì§€ ì‹ë³„
        fake_loss = loss_func_gan(out_dis,fake_label)

        d_loss = (real_loss + fake_loss) / 2.
        
        # íŒë³„ì(discriminator ì—…ë°ì´íŠ¸)
        d_loss.backward()
        opt_dis.step()

        loss_hist['gen'].append(g_loss.item())
        loss_hist['dis'].append(d_loss.item())

        batch_count += 1
        if batch_count % 100 == 0:
            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, g_loss.item(), d_loss.item(), (time.time()-start_time)/60))
```

    Epoch: 7, G_Loss: 32.751335, D_Loss: 0.012031, time: 1.50 min
    Epoch: 15, G_Loss: 31.956936, D_Loss: 0.004464, time: 3.04 min
    Epoch: 23, G_Loss: 28.797182, D_Loss: 0.004848, time: 4.61 min
    Epoch: 30, G_Loss: 28.570894, D_Loss: 0.000893, time: 6.17 min
    Epoch: 38, G_Loss: 25.091671, D_Loss: 0.007483, time: 7.72 min
    Epoch: 46, G_Loss: 22.723743, D_Loss: 0.001539, time: 9.26 min
    Epoch: 53, G_Loss: 21.941488, D_Loss: 0.000706, time: 10.83 min
    Epoch: 61, G_Loss: 21.969990, D_Loss: 0.000505, time: 12.37 min
    Epoch: 69, G_Loss: 21.019608, D_Loss: 0.000349, time: 13.91 min
    Epoch: 76, G_Loss: 19.570671, D_Loss: 0.000689, time: 15.46 min
    Epoch: 84, G_Loss: 20.565851, D_Loss: 0.000278, time: 17.02 min
    Epoch: 92, G_Loss: 18.415768, D_Loss: 0.000457, time: 18.56 min
    Epoch: 99, G_Loss: 20.046328, D_Loss: 0.205631, time: 20.10 min
    

discriminator loss ê²°ê³¼ê°’ì˜ ë§ˆì§€ë§‰ì„ ë³´ë©´ discriminatorì˜ lossê°’ì´ í•­ìƒ ì‘ë‹¤ê°€ 99ë²ˆì§¸ì¯¤ì— 500ë°°ì¯¤ ì»¤ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆëŠ”ë° ì´ê²ƒì„ ë³´ì•„ ì—í¬í¬ë¥¼ ëŠ˜ë¦¬ë©´ ì¢€ë” ì¢‹ì€ í•™ìŠµ ê²°ê³¼ê°€ ë‚˜ì˜¬ê²ƒì´ë¼ ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.

í•™ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ real_aëŠ” ì…ë ¥ ì´ë¯¸ì§€, real_bëŠ” ì¶œë ¥ í˜•íƒœì˜ ì‚¬ì§„ ì´ë¯¸ì§€ë¡œ ì„¤ì •í•˜ì˜€ê³  conditionalGANì˜ íŠ¹ì„±ìƒ generatorì™€ discriminator ë‘˜ ë‹¤ labelì„ ì‚¬ìš©í•´ í•™ìŠµí•˜ë¯€ë¡œ real_labelê³¼ fake_labelì„ ë§Œë“¤ì–´ ì£¼ì—ˆë‹¤.

ì‹ ê²½ë§ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ì„œ ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¨ëŒ€ë¡œ Dì™€ Gë¥¼ ë²ˆê°ˆì•„ê°€ë©° gradient descent stepì„ ì§„í–‰í•˜ì˜€ë‹¤.

### Generator í•™ìŠµ ë¶€ë¶„  

ì…ë ¥ ì´ë¯¸ì§€ë¥¼ generator modelì— ë„£ì–´ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ê²ƒì„ fake_bì— ì €ì¥í•˜ê³ , ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ì¶œë ¥ í˜•íƒœì˜ ì‚¬ì§„ì„ discriminator modelì— ë„£ì€, ì¦‰ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì‹ë³„í•œê±¸ out_disì— ì €ì¥í•©ë‹ˆë‹¤. out_disì™€ real_labelì€ MSElossë¡œ, ë§Œë“¤ì–´ì§„ ê²°ê³¼ê°€ ì‹¤ì œ ì‚¬ì§„ê³¼ ìœ ì‚¬í•  ìˆ˜ ìˆë„ë¡ fake_bì™€ real_bëŠ” L1 lossë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤. ê·¸ë¦¬í•˜ì—¬ ìµœì¢… lossëŠ” generator lossëŠ” gen_loss + pixel_loss * ê°€ì¤‘ì¹˜ì˜ í˜•íƒœë¡œ êµ¬ì„±.

### Discriminator í•™ìŠµ ë¶€ë¶„  

real_bì™€ real_aë¡œ ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì‹ë³„í•œ ê²ƒì„ out_disì— ì €ì¥í•˜ì—¬ real_labelê³¼ MSElossë¡œ í•™ìŠµí•œ ê²ƒì„ real_lossì— ì €ì¥í•˜ê³ , fake_bì™€ real_aë¡œ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì‹ë³„í•œê±¸ out_disì— ì €ì¥í•˜ì—¬ fake_labelê³¼ MSElossë¡œ í•™ìŠµí•œ ê²ƒì„ fake_lossì— ì €ì¥í–ˆë‹¤. ë”°ë¼ì„œ discciminator lossëŠ” real_loss ì™€ fake_lossë¥¼ ë”í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ëŠ”ë°, ì´ëŠ” discriminatorê°€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•´ ë‚´ë ¤ë©´ ì§„ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ë°ì´í„°ë„ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. <font color='green'>(discriminatorì˜ lossì‹ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ìë©´ ì§„ì§œ ì‚¬ì§„ì— ëŒ€í•´ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ì™€ ìƒì„±ëœ ì‚¬ì§„ì— ëŒ€í•´ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ì˜ lossí•©ì´ë¼ ë³´ë©´ ë ê²ƒê°™ë‹¤.)</font> ë˜í•œ Dë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë™ì•ˆ discriminator lossë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ, Gì˜ í•™ìŠµ ì†ë„ì— ë§ê²Œ í•˜ì˜€ë‹¤.


```python
# loss history
plt.figure(figsize=(10,5))
plt.title('Loss Progress')
plt.plot(loss_hist['gen'], label='Gen. Loss')
plt.plot(loss_hist['dis'], label='Dis. Loss')
plt.xlabel('batch count')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


    
![image](https://user-images.githubusercontent.com/93988405/213350535-63319da9-5d5c-42a4-a6f3-25293b762024.png)

    



```python
# ê°€ì¤‘ì¹˜ ì €ì¥
path2models = './models/'
os.makedirs(path2models, exist_ok=True)
path2weights_gen = os.path.join(path2models, 'weights_gen.pt')
path2weights_dis = os.path.join(path2models, 'weights_dis.pt')

torch.save(model_gen.state_dict(), path2weights_gen)
torch.save(model_dis.state_dict(), path2weights_dis)
```


```python
## Generatorê°€ ìƒì„±í•œ ê°€ì§œ ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°
```


```python
# ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
weights = torch.load(path2weights_gen)
model_gen.load_state_dict(weights)
```




    <All keys matched successfully>




```python
# evaluation model
model_gen.eval()

# ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±
with torch.no_grad():
    for a,b in train_dl:
        fake_imgs = model_gen(a.to(device)).detach().cpu()
        real_imgs = b
        break
```


```python
# ê°€ì§œ ì´ë¯¸ì§€ ì‹œê°í™”
plt.figure(figsize=(10,10))

for ii in range(0,16,2):
    plt.subplot(4,4,ii+1)
    plt.imshow(to_pil_image(0.5*real_imgs[ii]+0.5))
    plt.axis('off')
    plt.subplot(4,4,ii+2)
    plt.imshow(to_pil_image(0.5*fake_imgs[ii]+0.5))
    plt.axis('off')
```


    
![image](https://user-images.githubusercontent.com/93988405/213350570-09ec13aa-86f9-4887-94bd-ea6b15c2a841.png)

    

